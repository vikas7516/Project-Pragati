# PROJECT PRAGATI: TECHNICAL DOCUMENTATION
### System Architecture & Data Flow Reference
**Version**: 1.0 (Production)

---

## 1. High-Level Architecture
Project Pragati operates on a **Serverless, File-Based Architecture**. 
To ensure zero deployment costs and high scalability, we pre-compute all intelligence in Python and serve a static, optimized JSON payload to the Frontend.

### The Data Pipeline Flow
```mermaid
graph TD
    A[Raw Daily Logs (CSVs)] -->|Ingest| B(build_master.py)
    B -->|Aggregates| C[master_dataset.csv]
    C -->|Feeds| D(run_analysis.py)
    D -->|Applies Logic| E[dashboard_data.json]
    E -->|Loads| F[Frontend (Browser)]
```

---

## 2. Phase 1: The ETL Pipeline (Extraction & Aggregation)
**Goal**: Convert messy, daily transaction logs into a clean Time-Series Dataset.

*   **Script**: `data_cleaner/build_master.py`
*   **Input Source**: `aadhar-data/` (Folder containing raw daily CSVs).
*   **Key Operations**:
    1.  **Crawling**: Recursive search for `.csv` files.
    2.  **Categorization**: Identifies file type (Enrolment, Biometric, or Demographic) based on filename patterns.
    3.  **Normalization**: Standardizes column names (`pincode`, `date`, `count`).
    4.  **15-Day Bucketing**: 
        *   Raw Data is Daily.
        *   We aggregate this into **15-Day Periods** (e.g., `2024-01-H1`, `2024-01-H2`).
        *   *Why?* Smooths out daily noise (Sundays, holidays) to reveal true trends.
*   **Output Artifact**: `processed/master_dataset.csv`
    *   *Role*: The "Golden Source" for all analysis.
    *   *Schema*: `pincode`, `period_id`, `enrolment`, `biometric`, `demographic`.

---

## 3. Phase 2: The Intelligence Engine (Analysis)
**Goal**: Diagnostics. Determine the "Health Status" of every pincode.

*   **Script**: `processing_engine/run_analysis.py` (Orchestrator) & `analyzer.py` (Core Logic).
*   **Input**: `processed/master_dataset.csv`.
*   **Core Methodology (Vectorized)**:
    1.  **Capacity Profiling**: calculates the **90th Percentile** volume for each pincode over the last year. This establishes the "Max Potential".
    2.  **Volatility Analysis (IQR)**: Uses Inter-Quartile Range to determine a dynamic "Safe Band" for every specific center.
    3.  **The 5-Color Logic**:
        *   **CRITICAL (Red)**: Volume < 10% of Max Capacity (Collapse).
        *   **SEVERE (Orange)**: Volume < 30% of Max Capacity (Failing).
        *   **SHOCK (Yellow)**: Volume < Safe Floor (Statistical Anomaly).
        *   **SURGE (Blue)**: Volume > Safe Ceiling (Overload).
        *   **SAFE (Green)**: Within bounds.
*   **Output Artifact**: `frontend/data/dashboard_data.json`
    *   *Role*: The "Database" for the web app.
    *   *Size*: ~12MB (Optimized).

---

## 4. Phase 3: The Frontend Architecture
**Goal**: Zero-Latency Visualization.

*   **Technology**: Vanilla JavaScript (ES6 Modules) + Chart.js.
*   **Why No Database?** 
    By pre-computing the JSON, we eliminate the need for a backend server (SQL/Node). The entire app runs in the user's browser, making it incredibly fast and deployable on any static host (Cloudflare/GitHub Pages).

### File Structure & Logic
1.  **`index.html`**: The skeleton. Loads modules.
2.  **`js/main.js`**: The entry point. Triggers parallel loading of JSON and Map Data.
3.  **`js/state.js`**: A centralized "State Store" that holds the currently filtered view.
4.  **`js/map.js`**:
    *   Loads `india_pincode_boundaries_simplified.geojson`.
    *   Joins GeoJSON with `dashboard_data.json` using the Pincode as the Key.
    *   Colors the polygons based on the `priority` field (Red/Orange/Green).
5.  **`js/details.js`**:
    *   Renders the "Trend Velocity" charts using the historical arrays found in the JSON.
    *   Displays the specific "Diagnosis" text generated by the Python Engine.

---

## 5. Artifact Summary
| File Name | Generated By | Consumed By | Purpose |
| :--- | :--- | :--- | :--- |
| `master_dataset.csv` | `build_master.py` | `run_analysis.py` | Clean Time-Series History |
| `dashboard_data.json` | `run_analysis.py` | `frontend/js/api.js` | The "Brain" of the Dashboard |
| `pincode-adress.csv` | Extracted from Data | `frontend` | Provides District/State names |
